{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install stop-words\n",
    "!pip install transformers datasets imblearn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/datnlp/final_full_data_main.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(\"Count of tweets with label 1.0 (sarcastic):\", label_counts[1.0])\n",
    "print(\"Count of tweets with label 0.0 (non-sarcastic):\", label_counts[0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainf, X_test, y_trainf, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=seed, stratify=df['label']\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trainf, y_trainf, test_size=0.1, random_state=seed, stratify=y_trainf\n",
    ")\n",
    "\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "X_valid = X_valid.astype(str)\n",
    "\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)\n",
    "y_valid = pd.Series(y_valid).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Language Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tokens = {\n",
    "    \"hindi\": \"<2hi>\",\n",
    "    \"bengali\": \"<2bn>\",\n",
    "    \"english\": \"<2en>\",\n",
    "}\n",
    "\n",
    "def text_is_hindi(text):\n",
    "    return any(\"\\u0900\" <= char <= \"\\u097F\" for char in text)\n",
    "\n",
    "def text_is_bengali(text):\n",
    "    return any(\"\\u0980\" <= char <= \"\\u09FF\" for char in text)\n",
    "\n",
    "def assign_language_token(text):\n",
    "    if text_is_hindi(text):\n",
    "        return f\"{text} </s> {language_tokens['hindi']}\"\n",
    "    elif text_is_bengali(text):\n",
    "        return f\"{text} </s> {language_tokens['bengali']}\"\n",
    "    else:\n",
    "        return f\"{text} </s> {language_tokens['english']}\"\n",
    "\n",
    "# Apply language tokens\n",
    "X_train = X_train.apply(assign_language_token)\n",
    "X_valid = X_valid.apply(assign_language_token)\n",
    "X_test = X_test.apply(assign_language_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"xlm-roberta-large\", do_lower_case=False, use_fast=False, keep_accents=True\n",
    ")\n",
    "\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=150):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Class Imbalance Using Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=seed)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train.to_frame(), y_train)\n",
    "\n",
    "label_counts = y_train_resampled.value_counts()\n",
    "print(\"Count of tweets with label 1.0 (sarcastic) after sampling:\", label_counts[1.0])\n",
    "print(\"Count of tweets with label 0.0 (non-sarcastic) after sampling:\", label_counts[0.0])\n",
    "\n",
    "X_train_resampled = X_train_resampled['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SarcasmDataset(\n",
    "    texts=X_train_resampled.tolist(),\n",
    "    labels=y_train_resampled.tolist(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "valid_dataset = SarcasmDataset(\n",
    "    texts=X_valid.tolist(),\n",
    "    labels=y_valid.tolist(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "test_dataset = SarcasmDataset(\n",
    "    texts=X_test.tolist(),\n",
    "    labels=y_test.tolist(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-large\", num_labels=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=8)],\n",
    ")\n",
    "\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "\n",
    "epoch_train_losses = {}\n",
    "epoch_eval_losses = {}\n",
    "\n",
    "for entry in log_history:\n",
    "    if 'epoch' in entry:\n",
    "        epoch = int(entry['epoch'])\n",
    "        if 'loss' in entry:\n",
    "            epoch_train_losses[epoch] = entry['loss']\n",
    "        if 'eval_loss' in entry:\n",
    "            epoch_eval_losses[epoch] = entry['eval_loss']\n",
    "\n",
    "common_epochs = sorted(set(epoch_train_losses.keys()) & set(epoch_eval_losses.keys()))\n",
    "final_train_losses = [epoch_train_losses[e] for e in common_epochs]\n",
    "final_eval_losses = [epoch_eval_losses[e] for e in common_epochs]\n",
    "\n",
    "for epoch, (train_loss, eval_loss) in enumerate(zip(final_train_losses, final_eval_losses), start=1):\n",
    "    print(f\"Epoch {epoch}: Training Loss = {train_loss:.4f}, Validation Loss = {eval_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(common_epochs, final_train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(common_epochs, final_eval_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%', cmap=\"Reds\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
