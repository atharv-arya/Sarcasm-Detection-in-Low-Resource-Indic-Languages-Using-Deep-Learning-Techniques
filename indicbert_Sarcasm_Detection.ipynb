{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = pd.read_csv('bengali_dataset.csv')\n",
    "dfb = dfb.drop_duplicates(subset='text').reset_index(drop=True)\n",
    "dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = pd.read_csv('hindi_dataset.csv')\n",
    "dfh = dfh.drop_duplicates(subset='text').reset_index(drop=True)\n",
    "dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_stopwords=[\"অতএব\",\"অথচ\",\"অথবা\",\"অনুযায়ী\",\"অনেক\",\"অনেকে\",\"অনেকেই\",\"অন্তত\",\"অন্য\",\"অবধি\",\"অবশ্য\",\"অর্থাত\",\"আই\",\"আগামী\",\"আগে\",\n",
    "                  \"আগেই\",\"আছে\",\"আজ\",\"আদ্যভাগে\",\"আপনার\",\"আপনি\",\"আবার\",\"আমরা\",\"আমাকে\",\"আমাদের\",\"আমার\",\"আমি\",\"আর\",\"আরও\",\n",
    "                  \"ই\",\"ইত্যাদি\",\"ইহা\",\"উচিত\",\"উত্তর\",\"উনি\",\"উপর\",\"উপরে\",\"এ\",\"এঁদের\",\"এঁরা\",\"এই\",\"একই\",\"একটি\",\"একবার\",\"একে\",\"এক্\",\"এখন\",\n",
    "                  \"এখনও\",\"এখানে\",\"এখানেই\",\"এটা\",\"এটাই\",\"এটি\",\"এত\",\"এতটাই\",\"এতে\",\"এদের\",\"এব\",\"এবং\",\"এবার\",\"এমন\",\"এমনকী\",\"এমনি\",\"এর\",\n",
    "                  \"এরা\",\"এল\",\"এস\",\"এসে\",\"ঐ\",\"ও\",\"ওঁদের\",\"ওঁর\",\"ওঁরা\",\"ওই\",\"ওকে\",\"ওখানে\",\"ওদের\",\"ওর\",\"ওরা\",\"কখনও\",\"কত\",\"কবে\",\"কমনে\",\n",
    "                  \"কয়েক\",\"কয়েকটি\",\"করছে\",\"করছেন\",\"করতে\",\"করবে\",\"করবেন\",\"করলে\",\"করলেন\",\"করা\",\"করাই\",\"করায়\",\"করার\",\"করি\",\"করিতে\",\"করিয়া\",\n",
    "                  \"করিয়ে\",\"করে\",\"করেই\",\"করেছিলেন\",\"করেছে\",\"করেছেন\",\"করেন\",\"কাউকে\",\"কাছ\",\"কাছে\",\"কাজ\",\"কাজে\",\"কারও\",\"কারণ\",\"কি\",\"কিংবা\",\"কিছু\",\n",
    "                  \"কিছুই\",\"কিন্তু\",\"কী\",\"কে\",\"কেউ\",\"কেউই\",\"কেখা\",\"কেন\",\"কোটি\",\"কোন\",\"কোনও\",\"কোনো\",\"ক্ষেত্রে\",\"কয়েক\",\"খুব\",\"গিয়ে\",\"গিয়েছে\",\"গিয়ে\",\n",
    "                  \"গুলি\",\"গেছে\",\"গেল\",\"গেলে\",\"গোটা\",\"চলে\",\"চান\",\"চায়\",\"চার\",\"চালু\",\"চেয়ে\",\"চেষ্টা\",\"ছাড়া\",\"ছাড়াও\",\"ছিল\",\"ছিলেন\",\"জন\",\"জনকে\",\"জনের\",\n",
    "                  \"জন্য\",\"জন্যওজে\",\"জানতে\",\"জানা\",\"জানানো\",\"জানায়\",\"জানিয়ে\",\"জানিয়েছে\",\"জে\",\"জ্নজন\",\"টি\",\"ঠিক\",\"তখন\",\"তত\",\"তথা\",\"তবু\",\"তবে\",\n",
    "                  \"তা\",\"তাঁকে\",\"তাঁদের\",\"তাঁর\",\"তাঁরা\",\"তাঁাহারা\",\"তাই\",\"তাও\",\"তাকে\",\"তাতে\",\"তাদের\",\"তার\",\"তারপর\",\"তারা\",\"তারৈ\",\"তাহলে\",\"তাহা\",\"তাহাতে\",\n",
    "                  \"তাহার\",\"তিনঐ\",\"তিনি\",\"তিনিও\",\"তুমি\",\"তুলে\",\"তেমন\",\"তো\",\"তোমার\",\"থাকবে\",\"থাকবেন\",\"থাকা\",\"থাকায়\",\"থাকে\",\"থাকেন\",\"থেকে\",\"থেকেই\",\n",
    "                  \"থেকেও\",\"দিকে\",\"দিতে\",\"দিন\",\"দিয়ে\",\"দিয়েছে\",\"দিয়েছেন\",\"দিলেন\",\"দু\",\"দুই\",\"দুটি\",\"দুটো\",\"দেওয়া\",\"দেওয়ার\",\"দেওয়া\",\"দেখতে\",\"দেখা\",\"দেখে\",\n",
    "                  \"দেন\",\"দেয়\",\"দ্বারা\",\"ধরা\",\"ধরে\",\"ধামার\",\"নতুন\",\"নয়\",\"না\",\"নাই\",\"নাকি\",\"নাগাদ\",\"নানা\",\"নিজে\",\"নিজেই\",\"নিজেদের\",\"নিজের\",\"নিতে\",\"নিয়ে\",\n",
    "                  \"নিয়ে\",\"নেই\",\"নেওয়া\",\"নেওয়ার\",\"নেওয়া\",\"নয়\",\"পক্ষে\",\"পর\",\"পরে\",\"পরেই\",\"পরেও\",\"পর্যন্ত\",\"পাওয়া\",\"পাচ\",\"পারি\",\"পারে\",\"পারেন\",\"পি\",\"পেয়ে\",\n",
    "                  \"পেয়্র্\",\"প্রতি\",\"প্রথম\",\"প্রভৃতি\",\"প্রযন্ত\",\"প্রাথমিক\",\"প্রায়\",\"প্রায়\",\"ফলে\",\"ফিরে\",\"ফের\",\"বক্তব্য\",\"বদলে\",\"বন\",\"বরং\",\"বলতে\",\"বলল\",\"বললেন\",\"বলা\",\n",
    "                  \"বলে\",\"বলেছেন\",\"বলেন\",\"বসে\",\"বহু\",\"বা\",\"বাদে\",\"বার\",\"বি\",\"বিনা\",\"বিভিন্ন\",\"বিশেষ\",\"বিষয়টি\",\"বেশ\",\"বেশি\",\"ব্যবহার\",\"ব্যাপারে\",\"ভাবে\",\"ভাবেই\",\n",
    "                  \"মতো\",\"মতোই\",\"মধ্যভাগে\",\"মধ্যে\",\"মধ্যেই\",\"মধ্যেও\",\"মনে\",\"মাত্র\",\"মাধ্যমে\",\"মোট\",\"মোটেই\",\"যখন\",\"যত\",\"যতটা\",\"যথেষ্ট\",\"যদি\",\"যদিও\",\"যা\",\"যাঁর\",\n",
    "                  \"যাঁরা\",\"যাওয়া\",\"যাওয়ার\",\"যাওয়া\",\"যাকে\",\"যাচ্ছে\",\"যাতে\",\"যাদের\",\"যান\",\"যাবে\",\"যায়\",\"যার\",\"যারা\",\"যিনি\",\"যে\",\"যেখানে\",\"যেতে\",\"যেন\",\"যেমন\",\"র\",\n",
    "                  \"রকম\",\"রয়েছে\",\"রাখা\",\"রেখে\",\"লক্ষ\",\"শুধু\",\"শুরু\",\"সঙ্গে\",\"সঙ্গেও\",\"সব\",\"সবার\",\"সমস্ত\",\"সম্প্রতি\",\"সহ\",\"সহিত\",\"সাধারণ\",\"সামনে\",\"সি\",\"সুতরাং\",\"সে\",\n",
    "                  \"সেই\",\"সেখান\",\"সেখানে\",\"সেটা\",\"সেটাই\",\"সেটাও\",\"সেটি\",\"স্পষ্ট\",\"স্বয়ং\",\"হইতে\",\"হইবে\",\"হইয়া\",\"হওয়া\",\"হওয়ায়\",\"হওয়ার\",\"হচ্ছে\",\"হত\",\"হতে\",\"হতেই\",\n",
    "                  \"হন\",\"হবে\",\"হবেন\",\"হয়\",\"হয়তো\",\"হয়নি\",\"হয়ে\",\"হয়েই\",\"হয়েছিল\",\"হয়েছে\",\"হয়েছেন\",\"হল\",\"হলে\",\"হলেই\",\"হলেও\",\"হলো\",\"হাজার\",\"হিসাবে\",\"হৈলে\",\n",
    "                  \"হোক\",\"হয়\"]\n",
    "\n",
    "hindi_stop_words = get_stop_words('hi')\n",
    "english_stopwords= stopwords.words('english')\n",
    "\n",
    "stopwords_set = set(english_stopwords + hindi_stop_words + bangla_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = re.compile(r'((www\\.[\\S]+)|(https?://[\\S]+))')\n",
    "mention_pattern = re.compile(r'@\\w+')\n",
    "space_pattern = re.compile(r'\\s+')\n",
    "hashtag_pattern = re.compile(r'#(\\w+)')\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    text = url_pattern.sub('', text)\n",
    "    text = mention_pattern.sub('', text)\n",
    "    text = space_pattern.sub(' ', text)\n",
    "    text = hashtag_pattern.sub(r'\\1', text)\n",
    "    text = emoji.demojize(text)\n",
    "    #tokenized_text = nltk.word_tokenize(text) # check if tokenisation here is needed as bert tokeniser is used later\n",
    "    return ' '.join([word for word in text if word not in string.punctuation and word not in stopwords_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "dfb['text'] = dfb['text'].progress_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "dfh['text'] = dfh['text'].progress_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dfb, dfh], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(\"Count of tweets with label 1.0 (sarcastic):\", label_counts[1.0])\n",
    "print(\"Count of tweets with label 0.0 (non-sarcastic):\", label_counts[0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full, x_test, y_train_full, y_test = train_test_split(\n",
    "    df['text'].tolist(), df['label'], test_size=0.3, random_state=42, stratify=df['label']\n",
    ")\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "# Convert labels to Series and reset index\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "y_valid = pd.Series(y_valid).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "# Load IndicBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Define Dataset class for BERT\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=150,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset = SarcasmDataset(x_train, y_train)\n",
    "valid_dataset = SarcasmDataset(x_valid, y_valid)\n",
    "test_dataset = SarcasmDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IndicBART model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation strategy\n",
    "    save_strategy=\"epoch\",  # Save strategy to match evaluation strategy\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for the best model\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Stop training if validation loss does not improve for 2 evaluations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_results = trainer.train()\n",
    "\n",
    "# Store training and validation losses for plotting\n",
    "losses = trainer.state.log_history\n",
    "train_losses = [entry['loss'] for entry in losses if 'loss' in entry]\n",
    "eval_losses = [entry['eval_loss'] for entry in losses if 'eval_loss' in entry]\n",
    "\n",
    "# Predictions and performance metrics\n",
    "predictions = trainer.predict(valid_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "accuracy = np.mean(predicted_labels == y_test)\n",
    "print(f\"Accuracy Score: {accuracy}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss', color='blue')\n",
    "plt.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis for validation loss\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(eval_losses, label='Validation Loss', color='orange')\n",
    "ax2.set_ylabel('Validation Loss', color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Reds')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
