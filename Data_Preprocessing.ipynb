{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='/root/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = pd.read_csv('SarcasDetection.csv')\n",
    "dfb = dfb[['Comments', 'Label']]\n",
    "dfb.columns = ['text','label']\n",
    "dfb = dfb.drop_duplicates(subset='text').reset_index(drop=True)\n",
    "dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhs = pd.read_csv('Sarcasm_Hindi_Tweets-SARCASTIC.csv')\n",
    "dfhns = pd.read_csv('Sarcasm_Hindi_Tweets-NON-SARCASTIC.csv')\n",
    "dfhs = dfhs[['text']]\n",
    "dfhns = dfhns[['text']]\n",
    "dfhs['label'] = 1.0\n",
    "dfhns['label'] = 0.0\n",
    "dfh = pd.concat([dfhs, dfhns], axis=0)\n",
    "dfh = dfh.drop_duplicates(subset='text').reset_index(drop=True)\n",
    "dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhsr = pd.read_csv('S Reddit Hindi.csv')\n",
    "dfhnsr = pd.read_csv('NS Reddit Hindi.csv')\n",
    "dfhsr = dfhsr[['comment']]\n",
    "dfhnsr = dfhnsr[['comment']]\n",
    "dfhsr.columns = ['text']\n",
    "dfhnsr.columns = ['text']\n",
    "dfhsr['label'] = 1.0\n",
    "dfhnsr['label'] = 0.0\n",
    "dfhr = pd.concat([dfhsr, dfhnsr], axis=0)\n",
    "dfhr = dfhr.drop_duplicates(subset='text').reset_index(drop=True)\n",
    "dfhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbsr = pd.read_csv('S Reddit Bangla.csv')\n",
    "dfbnsr = pd.read_csv('NS Reddit Bangla.csv')\n",
    "dfbsr = dfbsr[['comment']]\n",
    "dfbnsr = dfbnsr[['comment']]\n",
    "dfbsr.columns = ['text']\n",
    "dfbnsr.columns = ['text']\n",
    "dfbsr['label'] = 1.0\n",
    "dfbnsr['label'] = 0.0\n",
    "dfbr = pd.concat([dfbsr, dfbnsr], axis=0)\n",
    "dfbr = dfbr.drop_duplicates(subset='text').reset_index(drop=True)\n",
    "dfbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_stopwords=[\"অতএব\",\"অথচ\",\"অথবা\",\"অনুযায়ী\",\"অনেক\",\"অনেকে\",\"অনেকেই\",\"অন্তত\",\"অন্য\",\"অবধি\",\"অবশ্য\",\"অর্থাত\",\"আই\",\"আগামী\",\"আগে\",\n",
    "                  \"আগেই\",\"আছে\",\"আজ\",\"আদ্যভাগে\",\"আপনার\",\"আপনি\",\"আবার\",\"আমরা\",\"আমাকে\",\"আমাদের\",\"আমার\",\"আমি\",\"আর\",\"আরও\",\n",
    "                  \"ই\",\"ইত্যাদি\",\"ইহা\",\"উচিত\",\"উত্তর\",\"উনি\",\"উপর\",\"উপরে\",\"এ\",\"এঁদের\",\"এঁরা\",\"এই\",\"একই\",\"একটি\",\"একবার\",\"একে\",\"এক্\",\"এখন\",\n",
    "                  \"এখনও\",\"এখানে\",\"এখানেই\",\"এটা\",\"এটাই\",\"এটি\",\"এত\",\"এতটাই\",\"এতে\",\"এদের\",\"এব\",\"এবং\",\"এবার\",\"এমন\",\"এমনকী\",\"এমনি\",\"এর\",\n",
    "                  \"এরা\",\"এল\",\"এস\",\"এসে\",\"ঐ\",\"ও\",\"ওঁদের\",\"ওঁর\",\"ওঁরা\",\"ওই\",\"ওকে\",\"ওখানে\",\"ওদের\",\"ওর\",\"ওরা\",\"কখনও\",\"কত\",\"কবে\",\"কমনে\",\n",
    "                  \"কয়েক\",\"কয়েকটি\",\"করছে\",\"করছেন\",\"করতে\",\"করবে\",\"করবেন\",\"করলে\",\"করলেন\",\"করা\",\"করাই\",\"করায়\",\"করার\",\"করি\",\"করিতে\",\"করিয়া\",\n",
    "                  \"করিয়ে\",\"করে\",\"করেই\",\"করেছিলেন\",\"করেছে\",\"করেছেন\",\"করেন\",\"কাউকে\",\"কাছ\",\"কাছে\",\"কাজ\",\"কাজে\",\"কারও\",\"কারণ\",\"কি\",\"কিংবা\",\"কিছু\",\n",
    "                  \"কিছুই\",\"কিন্তু\",\"কী\",\"কে\",\"কেউ\",\"কেউই\",\"কেখা\",\"কেন\",\"কোটি\",\"কোন\",\"কোনও\",\"কোনো\",\"ক্ষেত্রে\",\"কয়েক\",\"খুব\",\"গিয়ে\",\"গিয়েছে\",\"গিয়ে\",\n",
    "                  \"গুলি\",\"গেছে\",\"গেল\",\"গেলে\",\"গোটা\",\"চলে\",\"চান\",\"চায়\",\"চার\",\"চালু\",\"চেয়ে\",\"চেষ্টা\",\"ছাড়া\",\"ছাড়াও\",\"ছিল\",\"ছিলেন\",\"জন\",\"জনকে\",\"জনের\",\n",
    "                  \"জন্য\",\"জন্যওজে\",\"জানতে\",\"জানা\",\"জানানো\",\"জানায়\",\"জানিয়ে\",\"জানিয়েছে\",\"জে\",\"জ্নজন\",\"টি\",\"ঠিক\",\"তখন\",\"তত\",\"তথা\",\"তবু\",\"তবে\",\n",
    "                  \"তা\",\"তাঁকে\",\"তাঁদের\",\"তাঁর\",\"তাঁরা\",\"তাঁাহারা\",\"তাই\",\"তাও\",\"তাকে\",\"তাতে\",\"তাদের\",\"তার\",\"তারপর\",\"তারা\",\"তারৈ\",\"তাহলে\",\"তাহা\",\"তাহাতে\",\n",
    "                  \"তাহার\",\"তিনঐ\",\"তিনি\",\"তিনিও\",\"তুমি\",\"তুলে\",\"তেমন\",\"তো\",\"তোমার\",\"থাকবে\",\"থাকবেন\",\"থাকা\",\"থাকায়\",\"থাকে\",\"থাকেন\",\"থেকে\",\"থেকেই\",\n",
    "                  \"থেকেও\",\"দিকে\",\"দিতে\",\"দিন\",\"দিয়ে\",\"দিয়েছে\",\"দিয়েছেন\",\"দিলেন\",\"দু\",\"দুই\",\"দুটি\",\"দুটো\",\"দেওয়া\",\"দেওয়ার\",\"দেওয়া\",\"দেখতে\",\"দেখা\",\"দেখে\",\n",
    "                  \"দেন\",\"দেয়\",\"দ্বারা\",\"ধরা\",\"ধরে\",\"ধামার\",\"নতুন\",\"নয়\",\"না\",\"নাই\",\"নাকি\",\"নাগাদ\",\"নানা\",\"নিজে\",\"নিজেই\",\"নিজেদের\",\"নিজের\",\"নিতে\",\"নিয়ে\",\n",
    "                  \"নিয়ে\",\"নেই\",\"নেওয়া\",\"নেওয়ার\",\"নেওয়া\",\"নয়\",\"পক্ষে\",\"পর\",\"পরে\",\"পরেই\",\"পরেও\",\"পর্যন্ত\",\"পাওয়া\",\"পাচ\",\"পারি\",\"পারে\",\"পারেন\",\"পি\",\"পেয়ে\",\n",
    "                  \"পেয়্র্\",\"প্রতি\",\"প্রথম\",\"প্রভৃতি\",\"প্রযন্ত\",\"প্রাথমিক\",\"প্রায়\",\"প্রায়\",\"ফলে\",\"ফিরে\",\"ফের\",\"বক্তব্য\",\"বদলে\",\"বন\",\"বরং\",\"বলতে\",\"বলল\",\"বললেন\",\"বলা\",\n",
    "                  \"বলে\",\"বলেছেন\",\"বলেন\",\"বসে\",\"বহু\",\"বা\",\"বাদে\",\"বার\",\"বি\",\"বিনা\",\"বিভিন্ন\",\"বিশেষ\",\"বিষয়টি\",\"বেশ\",\"বেশি\",\"ব্যবহার\",\"ব্যাপারে\",\"ভাবে\",\"ভাবেই\",\n",
    "                  \"মতো\",\"মতোই\",\"মধ্যভাগে\",\"মধ্যে\",\"মধ্যেই\",\"মধ্যেও\",\"মনে\",\"মাত্র\",\"মাধ্যমে\",\"মোট\",\"মোটেই\",\"যখন\",\"যত\",\"যতটা\",\"যথেষ্ট\",\"যদি\",\"যদিও\",\"যা\",\"যাঁর\",\n",
    "                  \"যাঁরা\",\"যাওয়া\",\"যাওয়ার\",\"যাওয়া\",\"যাকে\",\"যাচ্ছে\",\"যাতে\",\"যাদের\",\"যান\",\"যাবে\",\"যায়\",\"যার\",\"যারা\",\"যিনি\",\"যে\",\"যেখানে\",\"যেতে\",\"যেন\",\"যেমন\",\"র\",\n",
    "                  \"রকম\",\"রয়েছে\",\"রাখা\",\"রেখে\",\"লক্ষ\",\"শুধু\",\"শুরু\",\"সঙ্গে\",\"সঙ্গেও\",\"সব\",\"সবার\",\"সমস্ত\",\"সম্প্রতি\",\"সহ\",\"সহিত\",\"সাধারণ\",\"সামনে\",\"সি\",\"সুতরাং\",\"সে\",\n",
    "                  \"সেই\",\"সেখান\",\"সেখানে\",\"সেটা\",\"সেটাই\",\"সেটাও\",\"সেটি\",\"স্পষ্ট\",\"স্বয়ং\",\"হইতে\",\"হইবে\",\"হইয়া\",\"হওয়া\",\"হওয়ায়\",\"হওয়ার\",\"হচ্ছে\",\"হত\",\"হতে\",\"হতেই\",\n",
    "                  \"হন\",\"হবে\",\"হবেন\",\"হয়\",\"হয়তো\",\"হয়নি\",\"হয়ে\",\"হয়েই\",\"হয়েছিল\",\"হয়েছে\",\"হয়েছেন\",\"হল\",\"হলে\",\"হলেই\",\"হলেও\",\"হলো\",\"হাজার\",\"হিসাবে\",\"হৈলে\",\n",
    "                  \"হোক\",\"হয়\"]\n",
    "\n",
    "hindi_stop_words = get_stop_words('hi')\n",
    "english_stopwords= stopwords.words('english')\n",
    "\n",
    "stopwords_set = set(english_stopwords + hindi_stop_words + bangla_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = re.compile(r'((www\\.[\\S]+)|(https?://[\\S]+))')\n",
    "mention_pattern = re.compile(r'@\\w+')\n",
    "space_pattern = re.compile(r'\\s+')\n",
    "hashtag_pattern = re.compile(r'#(\\w+)')\n",
    "\n",
    "# def process_text(text):\n",
    "#     text = text.lower()\n",
    "#     text = url_pattern.sub('', text)\n",
    "#     text = mention_pattern.sub('', text)\n",
    "#     text = space_pattern.sub(' ', text)\n",
    "#     text = hashtag_pattern.sub(r'\\1', text)\n",
    "#     text = emoji.demojize(text)\n",
    "#     #tokenized_text = nltk.word_tokenize(text) # check if tokenisation here is needed as bert tokeniser is used later\n",
    "#     return ' '.join([word for word in text if word not in string.punctuation and word not in stopwords_set])\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    text = url_pattern.sub('', text)\n",
    "    text = mention_pattern.sub('', text)\n",
    "    text = space_pattern.sub(' ', text)\n",
    "    text = hashtag_pattern.sub(r'\\1', text)\n",
    "    text = emoji.demojize(text)\n",
    "    words = text.split()  # Split text into words\n",
    "    return ' '.join([word for word in words if word not in string.punctuation and word not in stopwords_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "dfb['text'] = dfb['text'].progress_apply(process_text)\n",
    "dfh['text'] = dfh['text'].progress_apply(process_text)\n",
    "dfbr['text'] = dfbr['text'].progress_apply(process_text)\n",
    "dfhr['text'] = dfhr['text'].progress_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dfb, dfh, dfbr, dfhr], axis=0)\n",
    "output_file_path = 'final_full_data_main.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(\"Count of tweets with label 1.0 (sarcastic):\", label_counts[1.0])\n",
    "print(\"Count of tweets with label 0.0 (non-sarcastic):\", label_counts[0.0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
